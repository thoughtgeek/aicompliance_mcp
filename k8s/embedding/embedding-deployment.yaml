apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: embedding-models
  namespace: eu-ai-act
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: embedding-service
  namespace: eu-ai-act
spec:
  replicas: 1
  selector:
    matchLabels:
      app: embedding-service
  template:
    metadata:
      labels:
        app: embedding-service
    spec:
      containers:
      - name: embedding
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        command:
        - "bash"
        - "-c"
        - |
          set -e
          apt-get update && \
          apt-get install -y --no-install-recommends git && \
          apt-get clean && \
          rm -rf /var/lib/apt/lists/*
          
          pip install --no-cache-dir torch==1.13.1 sentence-transformers==2.2.2 fastapi==0.103.1 uvicorn==0.23.2 pydantic==2.3.0
          
          # Create app directory
          mkdir -p /app /models
          
          # Download model in a memory-efficient way
          python -c "from sentence_transformers import SentenceTransformer; model = SentenceTransformer('paraphrase-MiniLM-L3-v2', cache_folder='/models')"
          
          # Copy the app code
          cat > /app/app.py << 'EOL'
          #!/usr/bin/env python3
          """
          EU AI Act Embedding Service
          A lightweight API service to generate embeddings for text
          """
          
          import os
          import logging
          from typing import List, Dict, Any, Optional
          from fastapi import FastAPI, BackgroundTasks, HTTPException
          from pydantic import BaseModel
          import torch
          from sentence_transformers import SentenceTransformer
          import time
          import numpy as np
          
          # Setup logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger("embedding-service")
          
          # Configuration
          MODEL_NAME = os.environ.get("MODEL_NAME", "paraphrase-MiniLM-L3-v2")
          MODEL_PATH = os.environ.get("MODEL_PATH", f"/models/{MODEL_NAME}")
          DEFAULT_BATCH_SIZE = int(os.environ.get("DEFAULT_BATCH_SIZE", "16"))
          LOW_MEMORY_MODE = os.environ.get("LOW_MEMORY_MODE", "true").lower() == "true"
          USE_CPU = os.environ.get("USE_CPU", "true").lower() == "true"
          
          # Initialize the app
          app = FastAPI(
              title="EU AI Act Embedding Service",
              description="Lightweight text embedding generation service",
              version="1.0.0"
          )
          
          # Shared state
          state = {
              "model": None,
              "device": "cpu",
              "ready": False,
              "startup_time": None,
              "requests_processed": 0,
              "total_texts_embedded": 0,
              "total_processing_time": 0
          }
          
          # Request models
          class EmbeddingRequest(BaseModel):
              texts: List[str]
              batch_size: Optional[int] = None
          
          class EmbeddingResponse(BaseModel):
              embeddings: List[List[float]]
              processing_time: float
          
          # Background model loading
          def load_model_in_background():
              start_time = time.time()
              logger.info(f"Loading model {MODEL_NAME}...")
              
              try:
                  # Set device
                  if not USE_CPU and torch.cuda.is_available():
                      device = "cuda"
                      logger.info("Using CUDA for inference")
                  else:
                      device = "cpu"
                      logger.info("Using CPU for inference")
                  
                  state["device"] = device
                  
                  # Load model
                  if LOW_MEMORY_MODE:
                      logger.info("Loading model in low memory mode...")
                      # Use smaller quantized models or optimizations
                      model = SentenceTransformer(MODEL_NAME, device=device)
                  else:
                      model = SentenceTransformer(MODEL_NAME, device=device)
                      
                  # Save model
                  state["model"] = model
                  state["ready"] = True
                  state["startup_time"] = time.time() - start_time
                  
                  logger.info(f"Model loaded successfully in {state['startup_time']:.2f} seconds")
              except Exception as e:
                  logger.error(f"Failed to load model: {e}")
                  # Set a fallback model or strategy
                  state["ready"] = False
          
          # Embedding generation function
          def generate_embeddings(texts: List[str], batch_size: int = DEFAULT_BATCH_SIZE) -> List[List[float]]:
              """Generate embeddings for a list of texts"""
              if not state["ready"] or state["model"] is None:
                  logger.warning("Model not ready, using fallback strategy")
                  # Return zero embeddings as fallback
                  dim = 384  # all-MiniLM-L6-v2 dimension
                  return [[0.0] * dim for _ in texts]
              
              model = state["model"]
              
              # Process in batches to avoid OOM
              all_embeddings = []
              for i in range(0, len(texts), batch_size):
                  batch = texts[i:i+batch_size]
                  try:
                      # Generate embeddings
                      batch_embeddings = model.encode(batch, convert_to_tensor=False)
                      all_embeddings.extend(batch_embeddings.tolist())
                  except Exception as e:
                      logger.error(f"Error generating embeddings for batch {i}: {e}")
                      # Fallback for failed batch
                      dim = model.get_sentence_embedding_dimension()
                      all_embeddings.extend([[0.0] * dim for _ in batch])
              
              return all_embeddings
          
          # Routes
          @app.get("/")
          async def root():
              """Root endpoint with service info"""
              return {
                  "service": "EU AI Act Embedding Service",
                  "status": "ready" if state["ready"] else "loading",
                  "model": MODEL_NAME,
                  "device": state["device"],
                  "startup_time": state["startup_time"],
                  "requests_processed": state["requests_processed"],
                  "total_texts_embedded": state["total_texts_embedded"]
              }
          
          @app.get("/health")
          async def health():
              """Health check endpoint"""
              return {"status": "healthy" if state["ready"] else "initializing"}
          
          @app.get("/healthz")
          async def healthz():
              """Health check endpoint for Kubernetes probes"""
              return {"status": "healthy" if state["ready"] else "initializing"}
          
          @app.get("/livez")
          async def livez():
              """Liveness check endpoint for Kubernetes probes"""
              return {"status": "healthy" if state["ready"] else "initializing"}
          
          @app.get("/readyz")
          async def readyz():
              """Readiness check endpoint for Kubernetes probes"""
              return {"status": "healthy" if state["model"] is not None else "initializing"}
          
          @app.post("/embeddings", response_model=EmbeddingResponse)
          async def create_embeddings(request: EmbeddingRequest):
              """Generate embeddings for a list of texts"""
              if not state["ready"]:
                  if state["model"] is None:
                      # If model is still loading, return a temporary error
                      raise HTTPException(
                          status_code=503,
                          detail="Model is still loading, try again later"
                      )
              
              # Track stats
              state["requests_processed"] += 1
              state["total_texts_embedded"] += len(request.texts)
              
              # Use requested batch size or default
              batch_size = request.batch_size or DEFAULT_BATCH_SIZE
              
              # Generate embeddings
              start_time = time.time()
              embeddings = generate_embeddings(request.texts, batch_size)
              processing_time = time.time() - start_time
              
              # Track total processing time
              state["total_processing_time"] += processing_time
              
              # Return response
              return EmbeddingResponse(
                  embeddings=embeddings,
                  processing_time=processing_time
              )
          
          @app.on_event("startup")
          async def startup_event():
              """Load model on startup"""
              import asyncio
              asyncio.create_task(load_model_in_background())
          
          if __name__ == "__main__":
              import uvicorn
              uvicorn.run("app:app", host="0.0.0.0", port=8000, log_level="info")
          EOL
          
          # Run the service
          cd /app
          chmod +x app.py
          python -m uvicorn app:app --host 0.0.0.0 --port 8000
        env:
        - name: MODEL_NAME
          value: "paraphrase-MiniLM-L3-v2"
        - name: MODEL_PATH
          value: "/models"
        - name: DEFAULT_BATCH_SIZE
          value: "8"
        - name: LOW_MEMORY_MODE
          value: "true"
        - name: USE_CPU
          value: "true"
        ports:
        - containerPort: 8000
          name: http
        resources:
          requests:
            cpu: 200m
            memory: 750Mi
          limits:
            cpu: 500m
            memory: 1.5Gi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: embedding-models
          mountPath: /models
      volumes:
      - name: embedding-models
        persistentVolumeClaim:
          claimName: embedding-models
---
apiVersion: v1
kind: Service
metadata:
  name: embedding-service
  namespace: eu-ai-act
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: embedding-service
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: embedding-service-hpa
  namespace: eu-ai-act
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: embedding-service
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70 