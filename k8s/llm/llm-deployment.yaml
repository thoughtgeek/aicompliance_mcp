apiVersion: v1
kind: Secret
metadata:
  name: llm-api-keys
  namespace: eu-ai-act
type: Opaque
stringData:
  openai-api-key: "your-openai-api-key"  # Replace with actual key or leave empty
  anthropic-api-key: "your-anthropic-api-key"  # Replace with actual key or leave empty
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
  namespace: eu-ai-act
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-service
  template:
    metadata:
      labels:
        app: llm-service
    spec:
      containers:
      - name: llm
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        command:
        - "bash"
        - "-c"
        - |
          set -e
          apt-get update && \
          apt-get install -y --no-install-recommends git && \
          apt-get clean && \
          rm -rf /var/lib/apt/lists/*
          
          pip install --no-cache-dir fastapi==0.103.1 uvicorn==0.23.2 pydantic==2.3.0 httpx==0.24.1
          
          # Create app directory
          mkdir -p /app
          
          # Copy the app code
          cat > /app/app.py << 'EOL'
          #!/usr/bin/env python3
          """
          EU AI Act LLM Service
          A lightweight API service for factually grounded LLM responses
          """
          
          import os
          import logging
          import json
          import asyncio
          import time
          from typing import List, Dict, Any, Optional, Union
          from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request
          from fastapi.middleware.cors import CORSMiddleware
          from pydantic import BaseModel, Field
          import httpx
          import re
          
          # Setup logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger("llm-service")
          
          # Configuration
          DEFAULT_TEMPERATURE = float(os.environ.get("DEFAULT_TEMPERATURE", "0.1"))
          DEFAULT_MAX_TOKENS = int(os.environ.get("DEFAULT_MAX_TOKENS", "1024"))
          DEFAULT_MODEL = os.environ.get("DEFAULT_MODEL", "gpt-3.5-turbo")
          OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
          ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
          OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://ollama:11434")
          TERMINUSDB_URL = os.environ.get("TERMINUSDB_URL", "http://terminusdb:6363")
          VECTOR_DB_URL = os.environ.get("VECTOR_DB_URL", "http://vector-db:6333")
          EMBEDDING_URL = os.environ.get("EMBEDDING_URL", "http://embedding-service:8000")
          
          # Initialize the app
          app = FastAPI(
              title="EU AI Act LLM Service",
              description="Lightweight LLM proxy service with factual grounding",
              version="1.0.0"
          )
          
          # Add CORS middleware
          app.add_middleware(
              CORSMiddleware,
              allow_origins=["*"],
              allow_credentials=True,
              allow_methods=["*"],
              allow_headers=["*"],
          )
          
          # Shared state
          state = {
              "ready": True,
              "requests_processed": 0,
              "total_tokens_generated": 0,
              "total_processing_time": 0,
              "provider_stats": {
                  "openai": {"successful_requests": 0, "failed_requests": 0},
                  "anthropic": {"successful_requests": 0, "failed_requests": 0},
                  "ollama": {"successful_requests": 0, "failed_requests": 0},
              }
          }
          
          # Request and response models
          class Document(BaseModel):
              id: str
              content: str
              score: Optional[float] = None
              source: Optional[str] = None
              type: Optional[str] = None
              metadata: Optional[Dict[str, Any]] = None
          
          class GenerateRequest(BaseModel):
              query: str
              documents: Optional[List[Document]] = Field(default_factory=list)
              model: Optional[str] = None
              temperature: Optional[float] = None
              max_tokens: Optional[int] = None
              provider: Optional[str] = None
              stream: Optional[bool] = False
              system_prompt: Optional[str] = None
          
          class GenerateResponse(BaseModel):
              response: str
              model: str
              provider: str
              processing_time: float
              num_context_docs: int
              tokens_generated: Optional[int] = None
              sources: List[str] = Field(default_factory=list)
          
          class HealthResponse(BaseModel):
              status: str
              providers: Dict[str, bool]
          
          # Async HTTP client
          async def get_http_client():
              async with httpx.AsyncClient(timeout=60.0) as client:
                  yield client
          
          # LLM provider implementations
          class LLMProvider:
              """Base class for LLM providers"""
              async def generate(
                  self, 
                  query: str,
                  documents: List[Document],
                  model: str,
                  temperature: float,
                  max_tokens: int,
                  system_prompt: Optional[str] = None,
                  client: Optional[httpx.AsyncClient] = None
              ) -> Dict[str, Any]:
                  """Generate a response given query and context documents"""
                  raise NotImplementedError("Subclasses must implement generate method")
              
              async def health_check(self, client: httpx.AsyncClient) -> bool:
                  """Check if the provider is available"""
                  raise NotImplementedError("Subclasses must implement health_check method")
          
          class OpenAIProvider(LLMProvider):
              """OpenAI API provider"""
              def __init__(self, api_key: str):
                  self.api_key = api_key
                  self.base_url = "https://api.openai.com/v1/chat/completions"
              
              async def generate(
                  self, 
                  query: str,
                  documents: List[Document],
                  model: str = "gpt-3.5-turbo",
                  temperature: float = 0.1,
                  max_tokens: int = 1024,
                  system_prompt: Optional[str] = None,
                  client: Optional[httpx.AsyncClient] = None
              ) -> Dict[str, Any]:
                  """Generate a response using OpenAI API"""
                  if not self.api_key:
                      raise ValueError("OpenAI API key not configured")
                  
                  if not client:
                      async with httpx.AsyncClient(timeout=60.0) as client:
                          return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
                  else:
                      return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
                  
              async def _generate(
                  self,
                  client: httpx.AsyncClient,
                  query: str,
                  documents: List[Document],
                  model: str,
                  temperature: float,
                  max_tokens: int,
                  system_prompt: Optional[str]
              ) -> Dict[str, Any]:
                  """Internal method for OpenAI generation"""
                  context = "\n\n".join([f"Document {i+1}:\n{doc.content}" for i, doc in enumerate(documents)])
                  
                  if not system_prompt:
                      system_prompt = """You are an expert on the EU AI Act regulations. 
                      Use ONLY the information in the provided documents to answer the question.
                      If the necessary information is not present in the documents, say "I don't have enough information to answer this question."
                      Always cite the specific documents you used in your answer."""
                      
                  messages = [
                      {"role": "system", "content": system_prompt},
                      {"role": "user", "content": f"Context information:\n{context}\n\nQuestion: {query}"}
                  ]
                  
                  headers = {
                      "Content-Type": "application/json",
                      "Authorization": f"Bearer {self.api_key}"
                  }
                  
                  payload = {
                      "model": model,
                      "messages": messages,
                      "temperature": temperature,
                      "max_tokens": max_tokens
                  }
                  
                  response = await client.post(self.base_url, json=payload, headers=headers)
                  
                  if response.status_code != 200:
                      raise HTTPException(status_code=response.status_code, detail=f"OpenAI API error: {response.text}")
                  
                  result = response.json()
                  
                  return {
                      "response": result["choices"][0]["message"]["content"],
                      "model": model,
                      "provider": "openai",
                      "tokens_generated": result["usage"]["completion_tokens"],
                      "sources": [doc.id for doc in documents if doc.id]
                  }
              
              async def health_check(self, client: httpx.AsyncClient) -> bool:
                  """Check if OpenAI API is available"""
                  if not self.api_key:
                      return False
                      
                  try:
                      headers = {
                          "Content-Type": "application/json",
                          "Authorization": f"Bearer {self.api_key}"
                      }
                      
                      response = await client.get("https://api.openai.com/v1/models", headers=headers)
                      
                      return response.status_code == 200
                  except Exception as e:
                      logger.error(f"OpenAI health check failed: {e}")
                      return False
          
          class AnthropicProvider(LLMProvider):
              """Anthropic API provider"""
              def __init__(self, api_key: str):
                  self.api_key = api_key
                  self.base_url = "https://api.anthropic.com/v1/messages"
              
              async def generate(
                  self, 
                  query: str,
                  documents: List[Document],
                  model: str = "claude-2.1",
                  temperature: float = 0.1,
                  max_tokens: int = 1024,
                  system_prompt: Optional[str] = None,
                  client: Optional[httpx.AsyncClient] = None
              ) -> Dict[str, Any]:
                  """Generate a response using Anthropic API"""
                  if not self.api_key:
                      raise ValueError("Anthropic API key not configured")
                  
                  if not client:
                      async with httpx.AsyncClient(timeout=60.0) as client:
                          return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
                  else:
                      return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
              
              async def _generate(
                  self,
                  client: httpx.AsyncClient,
                  query: str,
                  documents: List[Document],
                  model: str,
                  temperature: float,
                  max_tokens: int,
                  system_prompt: Optional[str]
              ) -> Dict[str, Any]:
                  """Internal method for Anthropic generation"""
                  context = "\n\n".join([f"Document {i+1}:\n{doc.content}" for i, doc in enumerate(documents)])
                  
                  if not system_prompt:
                      system_prompt = """You are an expert on the EU AI Act regulations. 
                      Use ONLY the information in the provided documents to answer the question.
                      If the necessary information is not present in the documents, say "I don't have enough information to answer this question."
                      Always cite the specific documents you used in your answer."""
                  
                  headers = {
                      "Content-Type": "application/json",
                      "x-api-key": self.api_key,
                      "anthropic-version": "2023-06-01"
                  }
                  
                  payload = {
                      "model": model,
                      "messages": [{
                          "role": "user",
                          "content": f"Context information:\n{context}\n\nQuestion: {query}"
                      }],
                      "system": system_prompt,
                      "temperature": temperature,
                      "max_tokens": max_tokens
                  }
                  
                  response = await client.post(self.base_url, json=payload, headers=headers)
                  
                  if response.status_code != 200:
                      raise HTTPException(status_code=response.status_code, detail=f"Anthropic API error: {response.text}")
                  
                  result = response.json()
                  
                  return {
                      "response": result["content"][0]["text"],
                      "model": model,
                      "provider": "anthropic",
                      "tokens_generated": None,  # Anthropic doesn't return token count
                      "sources": [doc.id for doc in documents if doc.id]
                  }
              
              async def health_check(self, client: httpx.AsyncClient) -> bool:
                  """Check if Anthropic API is available"""
                  if not self.api_key:
                      return False
                      
                  try:
                      headers = {
                          "Content-Type": "application/json",
                          "x-api-key": self.api_key,
                          "anthropic-version": "2023-06-01"
                      }
                      
                      # This is a minimal valid request that should return quickly
                      payload = {
                          "model": "claude-2.1",
                          "messages": [{"role": "user", "content": "Hello"}],
                          "max_tokens": 1
                      }
                      
                      response = await client.post(self.base_url, json=payload, headers=headers)
                      
                      return response.status_code == 200
                  except Exception as e:
                      logger.error(f"Anthropic health check failed: {e}")
                      return False
          
          class OllamaProvider(LLMProvider):
              """Ollama API provider"""
              def __init__(self, base_url: str):
                  self.base_url = base_url
              
              async def generate(
                  self, 
                  query: str,
                  documents: List[Document],
                  model: str = "llama2",
                  temperature: float = 0.1,
                  max_tokens: int = 1024,
                  system_prompt: Optional[str] = None,
                  client: Optional[httpx.AsyncClient] = None
              ) -> Dict[str, Any]:
                  """Generate a response using Ollama API"""
                  if not client:
                      async with httpx.AsyncClient(timeout=120.0) as client:  # Longer timeout for local models
                          return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
                  else:
                      return await self._generate(client, query, documents, model, temperature, max_tokens, system_prompt)
              
              async def _generate(
                  self,
                  client: httpx.AsyncClient,
                  query: str,
                  documents: List[Document],
                  model: str,
                  temperature: float,
                  max_tokens: int,
                  system_prompt: Optional[str]
              ) -> Dict[str, Any]:
                  """Internal method for Ollama generation"""
                  context = "\n\n".join([f"Document {i+1}:\n{doc.content}" for i, doc in enumerate(documents)])
                  
                  if not system_prompt:
                      system_prompt = """You are an expert on the EU AI Act regulations. 
                      Use ONLY the information in the provided documents to answer the question.
                      If the necessary information is not present in the documents, say "I don't have enough information to answer this question."
                      Always cite the specific documents you used in your answer."""
                  
                  # Construct a prompt in the format Ollama expects
                  prompt = f"{system_prompt}\n\nContext information:\n{context}\n\nQuestion: {query}"
                  
                  payload = {
                      "model": model,
                      "prompt": prompt,
                      "temperature": temperature,
                      "num_predict": max_tokens,
                      "raw": False
                  }
                  
                  try:
                      response = await client.post(f"{self.base_url}/api/generate", json=payload, timeout=120.0)
                      
                      if response.status_code != 200:
                          raise HTTPException(status_code=response.status_code, detail=f"Ollama API error: {response.text}")
                      
                      result = response.json()
                      
                      # Extract the response
                      generated_text = result.get("response", "")
                      
                      return {
                          "response": generated_text,
                          "model": model,
                          "provider": "ollama",
                          "tokens_generated": result.get("eval_count"),
                          "sources": [doc.id for doc in documents if doc.id]
                      }
                  except Exception as e:
                      # Fallback to a very simple response if Ollama is not available
                      # This allows the service to work even without Ollama
                      logger.error(f"Error using Ollama: {e}")
                      return {
                          "response": "I could not process this request due to a backend error. Please try again later or contact support.",
                          "model": model,
                          "provider": "ollama_fallback",
                          "tokens_generated": 0,
                          "sources": []
                      }
              
              async def health_check(self, client: httpx.AsyncClient) -> bool:
                  """Check if Ollama API is available"""
                  try:
                      response = await client.get(f"{self.base_url}/api/tags")
                      return response.status_code == 200
                  except Exception as e:
                      logger.error(f"Ollama health check failed: {e}")
                      return False
          
          # Provider registry
          providers = {
              "openai": OpenAIProvider(OPENAI_API_KEY) if OPENAI_API_KEY else None,
              "anthropic": AnthropicProvider(ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None,
              "ollama": OllamaProvider(OLLAMA_BASE_URL)
          }
          
          # Utility function to select provider
          async def select_provider(provider_name: Optional[str] = None, client: httpx.AsyncClient = None) -> LLMProvider:
              """Select a provider based on name or availability"""
              if provider_name and provider_name in providers and providers[provider_name]:
                  return providers[provider_name]
              
              # Try providers in order of preference
              for name in ["openai", "anthropic", "ollama"]:
                  provider = providers.get(name)
                  if provider:
                      try:
                          is_available = await provider.health_check(client)
                          if is_available:
                              return provider
                      except Exception as e:
                          logger.error(f"Error checking provider {name}: {e}")
              
              # Fallback to Ollama (assumed to be local/always available)
              if providers["ollama"]:
                  return providers["ollama"]
              
              raise HTTPException(status_code=503, detail="No LLM providers available")
          
          # Routes
          @app.get("/", response_model=Dict[str, Any])
          async def root():
              """Root endpoint with service info"""
              return {
                  "service": "EU AI Act LLM Service",
                  "status": "ready" if state["ready"] else "not ready",
                  "requests_processed": state["requests_processed"],
                  "total_tokens_generated": state["total_tokens_generated"],
                  "provider_stats": state["provider_stats"]
              }
          
          @app.get("/health", response_model=HealthResponse)
          async def health(client: httpx.AsyncClient = Depends(get_http_client)):
              """Health check endpoint"""
              provider_status = {}
              
              for name, provider in providers.items():
                  if provider:
                      try:
                          provider_status[name] = await provider.health_check(client)
                      except Exception:
                          provider_status[name] = False
                  else:
                      provider_status[name] = False
              
              status = "healthy" if any(provider_status.values()) else "unhealthy"
              
              return HealthResponse(
                  status=status,
                  providers=provider_status
              )
          
          @app.post("/generate", response_model=GenerateResponse)
          async def generate(
              request: GenerateRequest,
              client: httpx.AsyncClient = Depends(get_http_client)
          ):
              """Generate a response for a given query and context"""
              start_time = time.time()
              
              try:
                  # Track request
                  state["requests_processed"] += 1
                  
                  # Select provider
                  provider = await select_provider(request.provider, client)
                  
                  # Set defaults
                  model = request.model or DEFAULT_MODEL
                  temperature = request.temperature or DEFAULT_TEMPERATURE
                  max_tokens = request.max_tokens or DEFAULT_MAX_TOKENS
                  
                  # Generate response
                  result = await provider.generate(
                      query=request.query,
                      documents=request.documents,
                      model=model,
                      temperature=temperature,
                      max_tokens=max_tokens,
                      system_prompt=request.system_prompt,
                      client=client
                  )
                  
                  # Track stats
                  state["provider_stats"][result["provider"]]["successful_requests"] += 1
                  if result.get("tokens_generated"):
                      state["total_tokens_generated"] += result["tokens_generated"]
                  
                  # Calculate processing time
                  processing_time = time.time() - start_time
                  state["total_processing_time"] += processing_time
                  
                  # Return response
                  return GenerateResponse(
                      response=result["response"],
                      model=result["model"],
                      provider=result["provider"],
                      processing_time=processing_time,
                      num_context_docs=len(request.documents),
                      tokens_generated=result.get("tokens_generated"),
                      sources=result.get("sources", [])
                  )
              
              except Exception as e:
                  # Track failed request if provider was identified
                  if hasattr(e, "provider") and e.provider in state["provider_stats"]:
                      state["provider_stats"][e.provider]["failed_requests"] += 1
                  
                  logger.error(f"Error generating response: {e}")
                  raise HTTPException(status_code=500, detail=str(e))
          
          @app.post("/verify")
          async def verify_response(
              response: str, 
              documents: List[Document],
              client: httpx.AsyncClient = Depends(get_http_client)
          ):
              """Verify if a response is factually grounded in the provided documents"""
              # This is a placeholder for a more sophisticated verification system
              # In a production system, this would check factual consistency
              
              # Simple heuristic: check if response mentions document content
              relevant_docs = []
              total_docs = len(documents)
              
              for doc in documents:
                  # Extract key phrases from the document
                  key_phrases = re.findall(r'\b\w{5,}\b', doc.content.lower())
                  matches = sum(1 for phrase in key_phrases if phrase in response.lower())
                  
                  if matches > 0:
                      relevant_docs.append({
                          "id": doc.id,
                          "relevance_score": matches / len(key_phrases) if key_phrases else 0
                      })
              
              return {
                  "is_grounded": len(relevant_docs) > 0,
                  "confidence": len(relevant_docs) / total_docs if total_docs > 0 else 0,
                  "relevant_documents": relevant_docs
              }
          
          if __name__ == "__main__":
              import uvicorn
              uvicorn.run("app:app", host="0.0.0.0", port=8008, log_level="info")
          EOL
          
          # Run the service
          cd /app
          chmod +x app.py
          python -m uvicorn app:app --host 0.0.0.0 --port 8008
        env:
        - name: DEFAULT_TEMPERATURE
          value: "0.1"
        - name: DEFAULT_MAX_TOKENS
          value: "1024"
        - name: DEFAULT_MODEL
          value: "gpt-3.5-turbo"
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-api-keys
              key: openai-api-key
              optional: true
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-api-keys
              key: anthropic-api-key
              optional: true
        - name: OLLAMA_BASE_URL
          value: "http://ollama-service:11434"
        - name: TERMINUSDB_URL
          valueFrom:
            configMapKeyRef:
              name: eu-ai-act-config
              key: TERMINUSDB_URL
        - name: VECTOR_DB_URL
          valueFrom:
            configMapKeyRef:
              name: eu-ai-act-config
              key: VECTOR_DB_URL
        - name: EMBEDDING_URL
          valueFrom:
            configMapKeyRef:
              name: eu-ai-act-config
              key: EMBEDDING_URL
        ports:
        - containerPort: 8008
          name: http
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8008
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8008
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
  namespace: eu-ai-act
spec:
  type: ClusterIP
  ports:
  - port: 8008
    targetPort: 8008
    protocol: TCP
    name: http
  selector:
    app: llm-service
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-service-hpa
  namespace: eu-ai-act
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-service
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70 